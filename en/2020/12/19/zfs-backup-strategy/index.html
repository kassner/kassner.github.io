<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta charset="UTF-8"><title>My ZFS backup strategy | Rafael Kassner</title><meta name="robots" content="index,follow"/><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://www.kassner.com.br/atom.xml"/><link rel="canonical" href="https://www.kassner.com.br/en/2020/12/19/zfs-backup-strategy/"/><link rel="stylesheet" href="https://www.kassner.com.br/assets/css/main.css"/></head><body class="post"><div class="background"></div><div id="wrapper"> <header> <nav id="site-navigation" class="main-navigation" role="navigation"> <label for="drop" class="toggle">Menu</label> <input type="checkbox" id="drop"/><ul id="primary-menu" class="menu"><li class="menu-item"><a href="https://www.kassner.com.br">Rafael Kassner</a></li><li class="menu-item"><a href="https://www.kassner.com.br/projects/">Projects</a></li><li class="menu-item"><a href="https://www.kassner.com.br/about/">About</a></li></ul> </nav> </header><div class="pull-down"></div><div class="container"><div id="primary" class="content-area"> <main id="main" class="site-main" role="main"> <article lang="en" class="post type-post status-publish format-standard hentry"><h1 class="article-header"><a href="https://www.kassner.com.br/en/2020/12/19/zfs-backup-strategy/">My ZFS backup strategy</a></h1><div class="entry-meta"><div class="byline"> <span class="posted-on"><time class="entry-date published" datetime="2020-12-19T00:00:00+00:00">2020-12-19</time></span> <span class="language">English</span></div></div><div class="entry-content"><p>I am now doing some experiments running my own NAS at home (mostly out of boredom), and I went with a small solution that goes inside my <a href="https://www.ikea.com/se/sv/p/ikea-ps-skap-vit-10251451/">IKEA PS</a> with a Raspberry Pi 4, a couple of 1TB USB SATA disks and <a href="https://zfsonlinux.org/">ZFS on Linux</a> mirroring them. I have less than 200GB in data and a very stable 50Mbps uplink at home, so this post explains my strategy to backup my data in a remote location.</p><p>Before I even started, I had to decide what would be my backup strategy. Given I’m running this from a home connection, it wasn’t an option to do full backups every day, specially when my data rarely changes more than 50MB in a given day, so I had to think about something that would play nice with incremental backups.</p><p>I first researched tools for such tools, and was amazed by <a href="https://www.borgbackup.org/">Borg</a>, but given I planned to backup this directly to an S3/GCS/B2 bucket, I found too many limitations on it. I can’t have the <code class="language-plaintext highlighter-rouge">borg</code> binary running in the remote, so I either needed a full copy of the data locally or it had to download a lot of data from the bucket to be able to compute the incremental diff.</p><p>My second test was with ZFS. Seeing demos of <code class="language-plaintext highlighter-rouge">zfs send | ssh remote zfs recv</code> was very beautiful, but again, I won’t have a way to run ZFS on the remote. But I did a few tests with it that caught my attention. First, snapshot management in ZFS is awesome to manage changes across time. Second, you can pipe <code class="language-plaintext highlighter-rouge">zfs send</code> to a file. Third, while <code class="language-plaintext highlighter-rouge">zfs send</code> is commonly used to send a snapshot to another machine also running ZFS (<code class="language-plaintext highlighter-rouge">zfs send | ssh remote zfs recv</code>), when you pipe <code class="language-plaintext highlighter-rouge">zfs send</code> to a non-ZFS command (or file), it will dump <em>the entire filesystem</em>, essentially creating a full backup. Fourth, you can <code class="language-plaintext highlighter-rouge">zfs send</code> just the diff between two snapshots, even to a file. So I put all of them together to create this backup strategy.</p><h2 id="backup-strategy">Backup strategy</h2><p>Note: the command <code class="language-plaintext highlighter-rouge">backup-upload</code> below is an unpublished tool that will compress, encrypt and upload the file to a remote location.</p><p><strong>Creating the full backup</strong></p><p>I’ve imported some data and then created a snapshot with <code class="language-plaintext highlighter-rouge">zfs snapshot data@&lt;date&gt;</code> (i.e.: <code class="language-plaintext highlighter-rouge">zfs snapshot data@2020-08-24</code>). This snapshot was then uploaded to a remote location using <code class="language-plaintext highlighter-rouge">zfs send data@2020-08-24 | backup-upload full/2020-08-24</code>.</p><p><strong>Creating daily backups</strong></p><p>Every day, a cronjob creates a new snapshot and uploads to remote using <code class="language-plaintext highlighter-rouge">zfs send -I &lt;yesterdaySnapshot&gt; &lt;todaySnapshot&gt; | backup-upload daily/&lt;date&gt;</code>).</p><p><strong>Creating monthly backups</strong></p><p>Once a month, after the daily snapshot was created, a cronjob uploads it using <code class="language-plaintext highlighter-rouge">zfs send -I &lt;fullbackup&gt; &lt;todaySnapshot&gt; | backup-upload monthly/&lt;date&gt;</code>).</p><p>This cronjob will also mark for deletion older monthly and daily backups, except the full backup.</p><h2 id="restoration-strategy">Restoration strategy</h2><p>Backups are worthless if they can’t be restored. This is the strategy to restore the data from the remote location. This assumes the data in <em>both</em> my local disks can’t be trusted anymore, so I’ll restore the backups from remote into a brand new disk with an empty zpool.</p><p>While creating the monthly backups, I run <code class="language-plaintext highlighter-rouge">zfs send</code> against the <code class="language-plaintext highlighter-rouge">&lt;fullbackup&gt;</code>. This means that the restoration process needs 1) the full backup; 2) the latest <em>monthly</em> backup and 3) all daily backups since the last monthly backup. This way <em>most</em> of my data will be restored by obtaining just two files, while I still retain the daily granularity if my future self wishes to use it.</p><p>The <code class="language-plaintext highlighter-rouge">backup-download</code> command below is an unpublished tool that will download, decrypt and decompress the file from a remote location.</p><p>Commands:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>backup-download full/2020-08-24 | zfs recv data

backup-download monthly/2020-12-01 | zfs recv data

backup-download daily/2020-12-02 | zfs recv data
backup-download daily/2020-12-03 | zfs recv data
...
backup-download daily/2020-12-19 | zfs recv data
</code></pre></div></div><h2 id="outro">Outro</h2><p>This is just an experiment and this blog post is a live document that will be updated as I fine tune my strategy. I am yet to buy a new disk and attempt full restoration from remote (I did small scale tests only). I am not too worried about losing data at this point because I have another external disk, without ZFS, that I manually copy all the data into regularly. If you want to try this yourself be careful, as you can lose data.</p></div><div class="entry-footer"><p class="cat-links">Posted in <a href="/tags/linux/" rel="tag">linux</a></p></div> </article> </main></div></div></div><footer><div class="footer-content"></div><div class="footer"><div><div> <span class="copy">Copyright © 2010-2021 Rafael Kassner</span></div></div></div> </footer></body></html>